#!/usr/bin/env python3
"""
T√©l√©chargeur de S√©ances du Conseil d'√âtat VD

Ce script permet d'extraire automatiquement les informations des s√©ances du Conseil d'√âtat
du canton de Vaud depuis le site officiel.

Auteur: Hugues Le Gendre
Date: 2024
"""

import json
import os
import re
import sys
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Optional, Tuple
from urllib.parse import urljoin, urlparse

import requests
from bs4 import BeautifulSoup
import time
from config import (
    MAX_PAGES,
    OUTPUT_FOLDER,
    PAGE_DELAY,
    REQUEST_TIMEOUT,
    STOP_DATE,
    TARGET_URL,
    USER_AGENT,
    VERBOSE,
)
import logging


class TelechargeurSeancesVD:
    def __init__(self, output_folder=OUTPUT_FOLDER):
        """
        Initialise le t√©l√©chargeur de s√©ances.
        
        Args:
            output_folder (str): Dossier pour sauvegarder les donn√©es extraites
        """
        self.output_folder = Path(output_folder)
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': USER_AGENT
        })
        
        # Configuration de la journalisation
        self.setup_logging()
        
        # Cr√©er le dossier de sortie s'il n'existe pas
        self.output_folder.mkdir(exist_ok=True)
        
        # Nom du fichier JSON unique pour toutes les s√©ances
        self.seances_file = self.output_folder / "seances_conseil_etat.json"
        
        # Charger les s√©ances existantes
        self.existing_seances = self.load_existing_seances()
        
        self.logger.info(f"T√©l√©chargeur de s√©ances initialis√© avec le dossier de sortie : {self.output_folder}")
        self.logger.info(f"Fichier de s√©ances : {self.seances_file}")
        self.logger.info(f"S√©ances existantes charg√©es : {len(self.existing_seances)}")
    
    def setup_logging(self):
        """Configuration de la journalisation."""
        self.logger = logging.getLogger('SeancesDownloader')
        self.logger.setLevel(logging.INFO)
        
        # Cr√©er le formateur
        formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')
        
        # Gestionnaire console
        console_handler = logging.StreamHandler()
        console_handler.setFormatter(formatter)
        self.logger.addHandler(console_handler)
        
    def get_page_content(self, url):
        """
        R√©cup√®re le contenu d'une page web.
        
        Args:
            url (str): URL √† r√©cup√©rer
            
        Returns:
            str: Contenu HTML de la page
        """
        try:
            self.logger.info(f"R√©cup√©ration de la page : {url}")
            response = self.session.get(url, timeout=REQUEST_TIMEOUT)
            response.raise_for_status()
            self.logger.info("Page r√©cup√©r√©e avec succ√®s")
            return response.text
        except requests.RequestException as e:
            self.logger.error(f"Erreur lors de la r√©cup√©ration de {url} : {e}")
            return None
    
    def extract_seance_links(self, html_content, base_url):
        """
        Extrait les liens des s√©ances du Conseil d'√âtat avec leurs dates.
        
        Args:
            html_content (str): Contenu HTML √† analyser
            base_url (str): URL de base pour construire les URLs compl√®tes
            
        Returns:
            list: Liste des dictionnaires contenant l'URL et la date de chaque s√©ance
        """
        soup = BeautifulSoup(html_content, 'html.parser')
        seance_links = []
        
        # Pattern pour d√©tecter les liens de s√©ances
        # Format attendu: "S√©ance du Conseil d'Etat du [date]"
        seance_pattern = re.compile(r'S√©ance du Conseil d\'Etat du (\d{1,2}\s+\w+\s+\d{4})', re.IGNORECASE)
        
        # Trouver tous les liens
        links = soup.find_all('a', href=True)
        
        for link in links:
            link_text = link.get_text(strip=True)
            href = link.get('href')
            
            if href and link_text:
                # V√©rifier si le texte du lien correspond au pattern d'une s√©ance
                match = seance_pattern.search(link_text)
                if match:
                    date_str = match.group(1)
                    full_url = urljoin(base_url, href)
                    
                    # Parser la date
                    try:
                        # Convertir la date fran√ßaise en objet datetime
                        date_obj = self.parse_french_date(date_str)
                        formatted_date = date_obj.strftime('%Y-%m-%d')
                    except Exception as e:
                        self.logger.warning(f"Impossible de parser la date '{date_str}': {e}")
                        formatted_date = date_str
                    
                    seance_info = {
                        'url': full_url,
                        'date': formatted_date,
                        'date_originale': date_str,
                        'titre': link_text
                    }
                    
                    seance_links.append(seance_info)
                    self.logger.debug(f"S√©ance trouv√©e : {date_str} -> {full_url}")
        
        return seance_links
    
    def parse_french_date(self, date_str):
        """
        Parse une date fran√ßaise en objet datetime.
        
        Args:
            date_str (str): Date au format fran√ßais (ex: "18 juin 2025")
            
        Returns:
            datetime: Objet datetime correspondant
        """
        # Mapping des mois fran√ßais vers les num√©ros
        mois_mapping = {
            'janvier': 1, 'f√©vrier': 2, 'mars': 3, 'avril': 4,
            'mai': 5, 'juin': 6, 'juillet': 7, 'ao√ªt': 8,
            'septembre': 9, 'octobre': 10, 'novembre': 11, 'd√©cembre': 12
        }
        
        # Pattern pour extraire jour, mois et ann√©e
        pattern = r'(\d{1,2})\s+(\w+)\s+(\d{4})'
        match = re.search(pattern, date_str.lower())
        
        if match:
            jour = int(match.group(1))
            mois_nom = match.group(2)
            annee = int(match.group(3))
            
            if mois_nom in mois_mapping:
                mois = mois_mapping[mois_nom]
                return datetime(annee, mois, jour)
            else:
                raise ValueError(f"Mois non reconnu: {mois_nom}")
        else:
            raise ValueError(f"Format de date non reconnu: {date_str}")
    
    def load_existing_seances(self):
        """
        Charge les s√©ances existantes depuis le fichier JSON.
        
        Returns:
            dict: Dictionnaire des s√©ances existantes avec l'URL comme cl√©
        """
        if not self.seances_file.exists():
            self.logger.info("Aucun fichier de s√©ances existant trouv√©")
            return {}
        
        try:
            with open(self.seances_file, 'r', encoding='utf-8') as f:
                data = json.load(f)
                seances = data.get('seances', [])
                
                # Cr√©er un dictionnaire avec l'URL comme cl√© pour un acc√®s rapide
                existing_seances = {}
                for seance in seances:
                    existing_seances[seance['url']] = seance
                
                self.logger.info(f"Chargement de {len(existing_seances)} s√©ances existantes")
                return existing_seances
                
        except (json.JSONDecodeError, FileNotFoundError) as e:
            self.logger.warning(f"Erreur lors du chargement des s√©ances existantes : {e}")
            return {}
    
    def save_seances_to_json(self, seances):
        """
        Sauvegarde les informations des s√©ances dans le fichier JSON unique.
        
        Args:
            seances (list): Liste des informations des s√©ances
            
        Returns:
            str: Chemin du fichier sauvegard√©
        """
        # Trier les s√©ances par date
        seances_sorted = sorted(seances, key=lambda x: x['date'], reverse=True)
        
        data = {
            'metadonnees': {
                'url_source': TARGET_URL,
                'derniere_mise_a_jour': datetime.now().isoformat(),
                'total_seances': len(seances_sorted)
            },
            'seances': seances_sorted
        }
        
        with open(self.seances_file, 'w', encoding='utf-8') as f:
            json.dump(data, f, ensure_ascii=False, indent=2)
        
        self.logger.info(f"Donn√©es sauvegard√©es dans : {self.seances_file}")
        return str(self.seances_file)
    
    def merge_new_seances(self, seances, current_date):
        """
        Fusionne les nouvelles s√©ances avec les existantes.
        
        Args:
            seances (list): Liste des nouvelles s√©ances trouv√©es
            current_date (str): Date de d√©couverte pour cette session
            
        Returns:
            list: Liste compl√®te des s√©ances (existantes + nouvelles)
        """
        merged_seances = list(self.existing_seances.values())
        new_count = 0
        
        for seance in seances:
            if seance['url'] not in self.existing_seances:
                # Ajouter la date de d√©couverte
                seance['date_decouverte'] = current_date
                merged_seances.append(seance)
                new_count += 1
                self.logger.info(f"Nouvelle s√©ance trouv√©e : {seance['date']} - {seance['titre']}")
            else:
                self.logger.debug(f"S√©ance d√©j√† connue ignor√©e : {seance['date']} - {seance['titre']}")
        
        self.logger.info(f"Fusion termin√©e : {new_count} nouvelles s√©ances ajout√©es")
        return merged_seances

    def extract_pagination_links(self, html_content, base_url):
        """
        Extrait les liens de pagination pour naviguer vers les pages suivantes.
        
        Args:
            html_content (str): Contenu HTML √† analyser
            base_url (str): URL de base pour construire les URLs compl√®tes
            
        Returns:
            list: Liste des URLs des pages suivantes
        """
        soup = BeautifulSoup(html_content, 'html.parser')
        pagination_links = []
        
        # Chercher la pagination
        pagination_nav = soup.find('nav', {'aria-label': 'Pagination'})
        if not pagination_nav:
            self.logger.debug("Aucune pagination trouv√©e")
            return pagination_links
        
        # Chercher les liens "Page suivante" avec la classe sp√©cifique
        next_links = pagination_nav.find_all('a', class_='vd-pagination__link')
        
        for link in next_links:
            # V√©rifier si c'est un lien "Page suivante"
            link_text = link.get_text(strip=True).lower()
            if 'suivante' in link_text or 'next' in link_text:
                href = link.get('href')
                if href:
                    full_url = urljoin(base_url, href)
                    # √âviter les doublons
                    if full_url not in pagination_links:
                        pagination_links.append(full_url)
                        self.logger.debug(f"Lien de pagination trouv√© : {full_url}")
        
        # Si aucun lien "suivante" trouv√©, essayer d'extraire le num√©ro de page actuel
        # et construire le lien vers la page suivante
        if not pagination_links:
            # Chercher le num√©ro de page actuel dans l'URL ou dans le contenu
            current_page_match = re.search(r'page%5D=(\d+)', html_content)
            if current_page_match:
                current_page = int(current_page_match.group(1))
                next_page = current_page + 1
                # Construire l'URL de la page suivante
                next_url = f"{base_url}/actualites/decisions-du-conseil-detat?tx_vdsafarinet_safarinet%5Bcontroller%5D=Meeting&tx_vdsafarinet_safarinet%5Bpage%5D={next_page}"
                pagination_links.append(next_url)
                self.logger.debug(f"Lien de pagination construit : {next_url}")
        
        return pagination_links

    def should_stop_scraping(self, seance_date):
        """
        D√©termine si le scraping doit s'arr√™ter bas√© sur la date de la s√©ance.
        
        Args:
            seance_date (str): Date de la s√©ance au format 'YYYY-MM-DD'
            
        Returns:
            bool: True si le scraping doit s'arr√™ter
        """
        if STOP_DATE is None:
            return False
        
        try:
            seance_dt = datetime.strptime(seance_date, '%Y-%m-%d')
            stop_dt = datetime.strptime(STOP_DATE, '%Y-%m-%d')
            return seance_dt < stop_dt
        except ValueError as e:
            self.logger.warning(f"Erreur lors de la comparaison des dates : {e}")
            return False

    def scrape_seances(self, target_url=None):
        """
        M√©thode principale pour extraire les s√©ances du Conseil d'√âtat avec pagination.
        
        Args:
            target_url (str): URL de la page √† extraire (utilise TARGET_URL par d√©faut)
            
        Returns:
            dict: R√©sum√© de l'extraction
        """
        if target_url is None:
            target_url = TARGET_URL
            
        self.logger.info(f"D√©but de l'extraction des s√©ances depuis : {target_url}")
        
        # D√©finir la date de d√©couverte une seule fois pour toute la session
        current_date = datetime.now().isoformat()
        self.logger.info(f"Date de d√©couverte pour cette session : {current_date}")
        
        # D√©tection automatique de l'URL de base depuis l'URL cible
        parsed_url = urlparse(target_url)
        base_url = f"{parsed_url.scheme}://{parsed_url.netloc}"
        self.logger.info(f"URL de base d√©tect√©e : {base_url}")
        
        all_seances = []
        pages_processed = 0
        stop_reached = False
        current_url = target_url
        visited_urls = set()  # Pour √©viter les boucles infinies
        
        while current_url and pages_processed < MAX_PAGES and not stop_reached:
            # V√©rifier si l'URL a d√©j√† √©t√© visit√©e
            if current_url in visited_urls:
                self.logger.warning(f"URL d√©j√† visit√©e, arr√™t pour √©viter la boucle infinie : {current_url}")
                break
                
            visited_urls.add(current_url)
            pages_processed += 1  # Incr√©menter le compteur au d√©but du traitement de chaque page
            self.logger.info(f"Traitement de la page {pages_processed} : {current_url}")
            
            # R√©cup√©rer le contenu de la page
            html_content = self.get_page_content(current_url)
            if not html_content:
                self.logger.error(f"√âchec de la r√©cup√©ration du contenu de la page {pages_processed}")
                break
            
            # Extraire les liens des s√©ances
            page_seances = self.extract_seance_links(html_content, base_url)
            
            if not page_seances:
                self.logger.warning(f"Aucune s√©ance trouv√©e sur la page {pages_processed}")
                break
            
            self.logger.info(f"Nombre de s√©ances trouv√©es sur la page {pages_processed} : {len(page_seances)}")
            
            # V√©rifier si on doit s'arr√™ter bas√© sur la date
            page_seances_to_add = []
            for seance in page_seances:
                if self.should_stop_scraping(seance['date']):
                    self.logger.info(f"Date limite atteinte ({STOP_DATE}). S√©ance trouv√©e : {seance['date']}")
                    stop_reached = True
                    break
                page_seances_to_add.append(seance)
            
            # Ajouter les s√©ances de cette page √† la liste totale
            all_seances.extend(page_seances_to_add)
            
            # Sauvegarder le fichier JSON apr√®s chaque page
            if page_seances_to_add:
                merged_seances = self.merge_new_seances(all_seances, current_date)
                self.save_seances_to_json(merged_seances)
                self.logger.info(f"Fichier JSON sauvegard√© apr√®s la page {pages_processed} ({len(merged_seances)} s√©ances total)")
            
            if stop_reached:
                break
            
            # Extraire les liens de pagination pour la page suivante
            pagination_links = self.extract_pagination_links(html_content, base_url)
            
            if pagination_links:
                current_url = pagination_links[0]  # Prendre le premier lien de pagination
                
                # D√©lai entre les requ√™tes pour √™tre respectueux
                if PAGE_DELAY > 0:
                    self.logger.debug(f"Attente de {PAGE_DELAY} seconde(s) avant la prochaine requ√™te")
                    time.sleep(PAGE_DELAY)
            else:
                self.logger.info("Aucun lien de pagination trouv√©, fin du scraping")
                break
        
        if not all_seances:
            self.logger.warning("Aucune s√©ance trouv√©e sur toutes les pages")
            return {'success': True, 'seances': [], 'message': 'Aucune s√©ance trouv√©e'}
        
        self.logger.info(f"Total des s√©ances trouv√©es sur {pages_processed} pages : {len(all_seances)}")
        
        # Fusionner avec les s√©ances existantes (final)
        merged_seances = self.merge_new_seances(all_seances, current_date)
        
        # Sauvegarder dans le fichier JSON (final)
        json_file = self.save_seances_to_json(merged_seances)
        
        return {
            'success': True,
            'seances': merged_seances,  # Ensemble de toutes les s√©ances (existantes + nouvelles)
            'new_seances': [s for s in all_seances if s['url'] not in self.existing_seances],  # Seulement les nouvelles
            'json_file': json_file,
            'total_count': len(merged_seances),
            'new_count': len([s for s in all_seances if s['url'] not in self.existing_seances]),
            'pages_processed': pages_processed,
            'stop_reached': stop_reached
        }


def main():
    """Fonction principale."""
    downloader = TelechargeurSeancesVD()
    
    print("=== T√©l√©chargeur de S√©ances du Conseil d'√âtat VD ===")
    print(f"URL cible : {TARGET_URL}")
    print(f"Dossier de sortie : {OUTPUT_FOLDER}")
    print(f"Fichier de s√©ances : {downloader.seances_file}")
    print(f"Date limite d'arr√™t : {STOP_DATE if STOP_DATE else 'Aucune'}")
    print(f"Nombre maximum de pages : {MAX_PAGES}")
    print(f"D√©lai entre les pages : {PAGE_DELAY} seconde(s)")
    print()
    
    result = downloader.scrape_seances()
    
    if result['success']:
        print(f"‚úÖ Extraction r√©ussie !")
        print(f"üìä Total des s√©ances : {result['total_count']}")
        print(f"üÜï Nouvelles s√©ances ajout√©es : {result['new_count']}")
        print(f"üìÑ Pages trait√©es : {result['pages_processed']}")
        print(f"üìÅ Fichier JSON : {result['json_file']}")
        
        if result.get('stop_reached'):
            print(f"üõë Arr√™t anticip√© : date limite ({STOP_DATE}) atteinte")
        
        if result['new_seances']:
            print("\nüÜï Nouvelles s√©ances ajout√©es :")
            for i, seance in enumerate(result['new_seances'][:5]):
                print(f"  {i+1}. {seance['date']} - {seance['titre']}")
            
            if len(result['new_seances']) > 5:
                print(f"  ... et {len(result['new_seances']) - 5} autres")
        else:
            print(f"\n‚ÑπÔ∏è  Aucune nouvelle s√©ance ajout√©e")
    else:
        print(f"‚ùå √âchec de l'extraction : {result.get('error', 'Erreur inconnue')}")


if __name__ == "__main__":
    main() 